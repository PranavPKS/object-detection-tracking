{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8-yl-s-WKMG"
   },
   "source": [
    "# Real-time Multi-object detection and tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFSqkTCdWKMI"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hV4P5gyTWKMI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import cv2\n",
    "import time\n",
    "from multiprocessing import Queue, Pool\n",
    "from distutils.version import StrictVersion\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):\n",
    "  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r5FNuiRPWKMN"
   },
   "source": [
    "## Object detection imports\n",
    "Here are the imports from the object detection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "bm0_uNRnWKMN",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "utils/visualization_utils.py:26: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/home/nvidia/.local/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/nvidia/.local/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/nvidia/.local/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 499, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/nvidia/.local/lib/python2.7/site-packages/tornado/ioloop.py\", line 1073, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/nvidia/.local/lib/python2.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/nvidia/.local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 456, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/nvidia/.local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 486, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/nvidia/.local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 438, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/nvidia/.local/lib/python2.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/nvidia/.local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/nvidia/.local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/nvidia/.local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/nvidia/.local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/nvidia/.local/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/nvidia/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2724, in run_cell\n",
      "    self.events.trigger('post_run_cell')\n",
      "  File \"/home/nvidia/.local/lib/python2.7/site-packages/IPython/core/events.py\", line 74, in trigger\n",
      "    func(*args, **kwargs)\n",
      "  File \"/home/nvidia/.local/lib/python2.7/site-packages/ipykernel/pylab/backend_inline.py\", line 164, in configure_once\n",
      "    activate_matplotlib(backend)\n",
      "  File \"/home/nvidia/.local/lib/python2.7/site-packages/IPython/core/pylabtools.py\", line 315, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/home/nvidia/.local/lib/python2.7/site-packages/matplotlib/pyplot.py\", line 231, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/home/nvidia/.local/lib/python2.7/site-packages/matplotlib/__init__.py\", line 1422, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/home/nvidia/.local/lib/python2.7/site-packages/matplotlib/backends/__init__.py\", line 17, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  import matplotlib; matplotlib.use('Agg')  # pylint: disable=multiple-statements\n"
     ]
    }
   ],
   "source": [
    "from utils import label_map_util\n",
    "\n",
    "from utils import visualization_utils as vis_util\n",
    "#from imutils.video import WebcamVideoStream\n",
    "from imutils.video import FPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfn_tRFOWKMO"
   },
   "source": [
    "# Model preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_sEBLpVWKMQ"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VyPz_t8WWKMQ"
   },
   "outputs": [],
   "source": [
    "#MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
    "MODEL_NAME = 'ssd_mobilenet_v11'\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tf_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "_tf_config.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YBcB9QHLWKMU"
   },
   "source": [
    "## Load a (frozen) Tensorflow model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KezjCRVvWKMV"
   },
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1MVVTcLWKMW"
   },
   "source": [
    "## Loading label map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
    "NUM_CLASSES = 90\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "class WebcamVideoStream:\n",
    "    def __init__(self, src, width, height):\n",
    "        # initialize the video camera stream and read the first frame\n",
    "        # from the stream\n",
    "        self.stream = cv2.VideoCapture(src)\n",
    "        self.stream.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "        self.stream.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "        (self.grabbed, self.frame) = self.stream.read()\n",
    "        \n",
    "        # initialize the variable used to indicate if the thread should\n",
    "        # be stopped\n",
    "        self.stopped = False\n",
    "\n",
    "    def start(self):\n",
    "        # start the thread to read frames from the video stream\n",
    "        Thread(target=self.update, args=()).start()\n",
    "        return self\n",
    "\n",
    "    def update(self):\n",
    "        # keep looping infinitely until the thread is stopped\n",
    "        while True:\n",
    "            # if the thread indicator variable is set, stop the thread\n",
    "            if self.stopped:\n",
    "                self.stream.release()\n",
    "                return\n",
    "\n",
    "            # otherwise, read the next frame from the stream\n",
    "            (self.grabbed, self.frame) = self.stream.read()\n",
    "\n",
    "    def read(self):\n",
    "        # return the frame most recently read\n",
    "        return self.frame\n",
    "\n",
    "    def stop(self):\n",
    "        # indicate that the thread should be stopped\n",
    "        self.stopped = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_detect2track(box, width, height):\n",
    "    # transforms normalized to absolut coords\n",
    "    ymin, xmin, ymax, xmax = box\n",
    "    ymin = ymin*height\n",
    "    xmin = xmin*width\n",
    "    ymax = ymax*height\n",
    "    xmax = xmax*width\n",
    "    boxwidth= xmax - xmin\n",
    "    boxheight = ymax - ymin\n",
    "    \n",
    "    newbox = [xmin,ymin, boxwidth, boxheight]\n",
    "    #newbox = map(int,newbox)\n",
    "    return newbox\n",
    "\n",
    "def conv_track2detect(box, width, height):\n",
    "    # transforms absolut to normalized coords\n",
    "    dw = 1./width\n",
    "    dh = 1./height\n",
    "    x, y, boxwidth, boxheight = box #map(float,box)\n",
    "    xmin = x * dw\n",
    "    ymin = y * dh\n",
    "    xmax = (x+boxwidth) * dw\n",
    "    ymax = (y+boxheight) * dh\n",
    "    \n",
    "    newbox = np.array([ymin,xmin,ymax,xmax])\n",
    "    return newbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(input_q, output_q):\n",
    "    \n",
    "    import sys\n",
    "    sys.path.append(os.getcwd()+'/KCFcpp')\n",
    "    import KCF\n",
    "    tracker = KCF.kcftracker(False, True, False, False)\n",
    "    tracker_counter = 0\n",
    "    track = False\n",
    "\n",
    "    detection_graph = tf.Graph()\n",
    "    with detection_graph.as_default():\n",
    "        od_graph_def = tf.GraphDef()\n",
    "        with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "            serialized_graph = fid.read()\n",
    "            od_graph_def.ParseFromString(serialized_graph)\n",
    "            tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "        sess = tf.Session(graph=detection_graph,config= _tf_config)\n",
    "\n",
    "    fps = FPS().start()\n",
    "    frame_counter = 0\n",
    "    printer = False #Using this boolean to print only few frame's details as text\n",
    "    \n",
    "    while True:\n",
    "        if not track:\n",
    "            image_np = input_q.get()\n",
    "            frame_counter += 1\n",
    "            if frame_counter % 10 == 0:\n",
    "                printer = True\n",
    "\n",
    "            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "            detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "            detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "            detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "            #print(image_np.shape)\n",
    "              # to have a shape [1, None, None, 3]\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "              # Detection.\n",
    "            (boxes, scores, classes, num) = sess.run(\n",
    "                  [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "                  feed_dict={image_tensor: image_np_expanded})\n",
    "              # Visualization\n",
    "            boxes, classes, scores = np.squeeze(boxes),np.squeeze(classes).astype(np.int32),np.squeeze(scores)\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                  image_np,\n",
    "                  boxes, \n",
    "                  classes, \n",
    "                  scores,\n",
    "                  category_index,\n",
    "                  use_normalized_coordinates=True,\n",
    "                  line_thickness=3,\n",
    "                  min_score_thresh=.5)\n",
    "\n",
    "            for box, score, _class in zip(boxes, scores, classes):\n",
    "                if printer:\n",
    "                    label = category_index[_class]['name']\n",
    "                    print(\"label: {}\\nscore: {}\\nbox: {}\".format(label, score, box))\n",
    "                    printer = False\n",
    "\n",
    "            output_q.put(image_np)\n",
    "            \n",
    "            # Activate Tracker\n",
    "            if num <= 5: # Max number of objects to track\n",
    "                tracker_frame = image_np\n",
    "                track = True\n",
    "                first_track = True\n",
    "\n",
    "        # Tracking\n",
    "        else:\n",
    "            frame = input_q.get()\n",
    "            frame_counter += 1\n",
    "            if frame_counter % 10 == 0:\n",
    "                printer = True\n",
    "                \n",
    "            if first_track:\n",
    "                trackers = []\n",
    "                tracker_boxes = boxes\n",
    "                for box in boxes[~np.all(boxes == 0, axis=1)]:\n",
    "                        tracker.init(conv_detect2track(box,480, 360), tracker_frame)\n",
    "                        trackers.append(tracker)\n",
    "                first_track = False\n",
    "\n",
    "            for idx,tracker in enumerate(trackers):\n",
    "                tracker_box = tracker.update(frame)\n",
    "                tracker_boxes[idx,:] = conv_track2detect(tracker_box, 480, 360)\n",
    "\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                  frame,\n",
    "                  boxes, \n",
    "                  classes, \n",
    "                  scores,\n",
    "                  category_index,\n",
    "                  use_normalized_coordinates=True,\n",
    "                  line_thickness=3,\n",
    "                  min_score_thresh=.5)\n",
    "    \n",
    "            tracker_counter += 1\n",
    "            if tracker_counter >= 20: #Number of tracked frames between detections\n",
    "                track = False\n",
    "                tracker_counter = 0\n",
    "            \n",
    "            output_q.put(frame)\n",
    "            \n",
    "        fps.update()\n",
    "        \n",
    "    fps.stop()\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] sampling THREADED frames from webcam...\n",
      "label: person\n",
      "score: 0.787391662598\n",
      "box: [0.09397298 0.30995643 0.9922058  0.993363  ]\n",
      "label: person\n",
      "score: 0.790426909924\n",
      "box: [0.1174548  0.2920087  0.9933994  0.99393636]\n",
      "label: person\n",
      "score: 0.807669401169\n",
      "box: [0.12734544 0.34864846 0.9957831  0.99904966]\n",
      "label: person\n",
      "score: 0.895899713039\n",
      "box: [0.16184935 0.09494008 0.99676144 0.49622512]\n",
      "label: person\n",
      "score: 0.850935637951\n",
      "box: [0.1495997  0.12336215 0.99738336 0.52386606]\n",
      "label: person\n",
      "score: 0.761521577835\n",
      "box: [0.2262091 0.4102541 0.9885536 0.9978684]\n",
      "label: person\n",
      "score: 0.807542681694\n",
      "box: [0.23116577 0.41477644 0.98890626 0.9987943 ]\n",
      "label: person\n",
      "score: 0.753902256489\n",
      "box: [0.17485568 0.39481848 0.9898746  0.99578756]\n",
      "label: person\n",
      "score: 0.808250248432\n",
      "box: [0.20639667 0.4437852  0.9663031  0.9956881 ]\n",
      "label: person\n",
      "score: 0.875904262066\n",
      "box: [0.21151507 0.43510184 0.98342717 0.9979001 ]\n",
      "label: person\n",
      "score: 0.880375444889\n",
      "box: [0.22131747 0.43350065 0.9824597  0.99450004]\n",
      "label: person\n",
      "score: 0.853874564171\n",
      "box: [0.1901868  0.43567434 0.99608594 0.99734545]\n",
      "label: person\n",
      "score: 0.823498427868\n",
      "box: [0.1567299  0.42996794 0.9996375  0.9953392 ]\n",
      "label: person\n",
      "score: 0.737634956837\n",
      "box: [0.23934   0.4736346 1.        0.9965155]\n",
      "label: person\n",
      "score: 0.633621633053\n",
      "box: [0.2378551  0.46425483 1.         0.99680865]\n"
     ]
    }
   ],
   "source": [
    "input_q = Queue(maxsize=5)\n",
    "output_q = Queue(maxsize=5)\n",
    "pool = Pool(2, worker, (input_q, output_q))\n",
    "\n",
    "\n",
    "print('[INFO] sampling THREADED frames from webcam...')\n",
    "vs = WebcamVideoStream(src=0,width = 480,height=360).start()\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "\n",
    "out = cv2.VideoWriter('output.avi', fourcc, 5.0, (544, 288),True)\n",
    "frame_counter = 0\n",
    "\n",
    "fps = FPS().start()\n",
    "f_count = 0\n",
    "\n",
    "while True:\n",
    "    frame = vs.read()\n",
    "    input_q.put(frame)\n",
    "\n",
    "    t = time.time()\n",
    "    \n",
    "    if output_q.empty():\n",
    "        pass  # fill up queue\n",
    "    else:\n",
    "        output_rgb = output_q.get()\n",
    "        out.write(output_rgb)\n",
    "        cv2.imshow('Video', output_rgb)\n",
    "        f_count += 1\n",
    "        if f_count >= 1000: #For testing.. limiting the number of frames\n",
    "            break\n",
    "        \n",
    "    fps.update()\n",
    "\n",
    "    #print('[INFO] elapsed time: {:.2f}'.format(time.time() - t))\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "fps.stop()\n",
    "print('[INFO] elapsed time (total): {:.2f}'.format(fps.elapsed()))\n",
    "print('[INFO] approx. FPS: {:.2f}'.format(fps.fps()))\n",
    "\n",
    "pool.terminate()\n",
    "vs.stop()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "object_detection_tutorial.ipynb?workspaceId=ronnyvotel:python_inference::citc",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
