{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8-yl-s-WKMG"
   },
   "source": [
    "# Real-time Multi-object detection and tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFSqkTCdWKMI"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hV4P5gyTWKMI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import cv2\n",
    "import time\n",
    "from multiprocessing import Queue, Pool\n",
    "from distutils.version import StrictVersion\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):\n",
    "  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r5FNuiRPWKMN"
   },
   "source": [
    "## Object detection imports\n",
    "Here are the imports from the object detection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "bm0_uNRnWKMN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import label_map_util\n",
    "\n",
    "from utils import visualization_utils as vis_util\n",
    "#from imutils.video import WebcamVideoStream\n",
    "from imutils.video import FPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfn_tRFOWKMO"
   },
   "source": [
    "# Model preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_sEBLpVWKMQ"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VyPz_t8WWKMQ"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YBcB9QHLWKMU"
   },
   "source": [
    "## Load a (frozen) Tensorflow model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KezjCRVvWKMV"
   },
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1MVVTcLWKMW"
   },
   "source": [
    "## Loading label map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
    "NUM_CLASSES = 90\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "class WebcamVideoStream:\n",
    "    def __init__(self, src, width, height):\n",
    "        # initialize the video camera stream and read the first frame\n",
    "        # from the stream\n",
    "        self.stream = cv2.VideoCapture(src)\n",
    "        self.stream.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "        self.stream.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "        (self.grabbed, self.frame) = self.stream.read()\n",
    "        \n",
    "        # initialize the variable used to indicate if the thread should\n",
    "        # be stopped\n",
    "        self.stopped = False\n",
    "\n",
    "    def start(self):\n",
    "        # start the thread to read frames from the video stream\n",
    "        Thread(target=self.update, args=()).start()\n",
    "        return self\n",
    "\n",
    "    def update(self):\n",
    "        # keep looping infinitely until the thread is stopped\n",
    "        while True:\n",
    "            # if the thread indicator variable is set, stop the thread\n",
    "            if self.stopped:\n",
    "                self.stream.release()\n",
    "                return\n",
    "\n",
    "            # otherwise, read the next frame from the stream\n",
    "            (self.grabbed, self.frame) = self.stream.read()\n",
    "\n",
    "    def read(self):\n",
    "        # return the frame most recently read\n",
    "        return self.frame\n",
    "\n",
    "    def stop(self):\n",
    "        # indicate that the thread should be stopped\n",
    "        self.stopped = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_detect2track(box, width, height):\n",
    "    # transforms normalized to absolut coords\n",
    "    ymin, xmin, ymax, xmax = box\n",
    "    ymin = ymin*height\n",
    "    xmin = xmin*width\n",
    "    ymax = ymax*height\n",
    "    xmax = xmax*width\n",
    "    boxwidth= xmax - xmin\n",
    "    boxheight = ymax - ymin\n",
    "    \n",
    "    newbox = [xmin,ymin, boxwidth, boxheight]\n",
    "    #newbox = map(int,newbox)\n",
    "    return newbox\n",
    "\n",
    "def conv_track2detect(box, width, height):\n",
    "    # transforms absolut to normalized coords\n",
    "    dw = 1./width\n",
    "    dh = 1./height\n",
    "    x, y, boxwidth, boxheight = box #map(float,box)\n",
    "    xmin = x * dw\n",
    "    ymin = y * dh\n",
    "    xmax = (x+boxwidth) * dw\n",
    "    ymax = (y+boxheight) * dh\n",
    "    \n",
    "    newbox = np.array([ymin,xmin,ymax,xmax])\n",
    "    return newbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def worker(input_q, output_q):\n",
    "    \n",
    "    from KCFpy import kcftracker\n",
    "    tracker = kcftracker.KCFTracker(False, True, False)\n",
    "    tracker_counter = 0\n",
    "    track = False\n",
    "\n",
    "    detection_graph = tf.Graph()\n",
    "    with detection_graph.as_default():\n",
    "        od_graph_def = tf.GraphDef()\n",
    "        with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "            serialized_graph = fid.read()\n",
    "            od_graph_def.ParseFromString(serialized_graph)\n",
    "            tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "        sess = tf.Session(graph=detection_graph)\n",
    "\n",
    "    fps = FPS().start()\n",
    "    frame_counter = 0\n",
    "    printer = False #Using this boolean to print only few frame's details as text\n",
    "    \n",
    "    while True:\n",
    "        if not track:\n",
    "            image_np = input_q.get()\n",
    "            frame_counter += 1\n",
    "            if frame_counter % 10 == 0:\n",
    "                printer = True\n",
    "\n",
    "            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "            detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "            detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "            detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "            #print(image_np.shape)\n",
    "              # to have a shape [1, None, None, 3]\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "              # Detection.\n",
    "            (boxes, scores, classes, num) = sess.run(\n",
    "                  [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "                  feed_dict={image_tensor: image_np_expanded})\n",
    "              # Visualization\n",
    "            boxes, classes, scores = np.squeeze(boxes),np.squeeze(classes).astype(np.int32),np.squeeze(scores)\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                  image_np,\n",
    "                  boxes, \n",
    "                  classes, \n",
    "                  scores,\n",
    "                  category_index,\n",
    "                  use_normalized_coordinates=True,\n",
    "                  line_thickness=3,\n",
    "                  min_score_thresh=.5)\n",
    "\n",
    "            for box, score, _class in zip(boxes, scores, classes):\n",
    "                if printer:\n",
    "                    label = category_index[_class]['name']\n",
    "                    print(\"label: {}\\nscore: {}\\nbox: {}\".format(label, score, box))\n",
    "                    printer = False\n",
    "\n",
    "            output_q.put(image_np)\n",
    "            \n",
    "            # Activate Tracker\n",
    "            if num <= 5: # Max number of objects to track\n",
    "                tracker_frame = image_np\n",
    "                track = True\n",
    "                first_track = True\n",
    "\n",
    "        # Tracking\n",
    "        else:\n",
    "            frame = input_q.get()\n",
    "            frame_counter += 1\n",
    "            if frame_counter % 10 == 0:\n",
    "                printer = True\n",
    "                \n",
    "            if first_track:\n",
    "                trackers = []\n",
    "                tracker_boxes = boxes\n",
    "                for box in boxes[~np.all(boxes == 0, axis=1)]:\n",
    "                        tracker.init(conv_detect2track(box,480, 360), tracker_frame)\n",
    "                        trackers.append(tracker)\n",
    "                first_track = False\n",
    "\n",
    "            for idx,tracker in enumerate(trackers):\n",
    "                tracker_box = tracker.update(frame)\n",
    "                tracker_boxes[idx,:] = conv_track2detect(tracker_box, 480, 360)\n",
    "\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                  frame,\n",
    "                  boxes, \n",
    "                  classes, \n",
    "                  scores,\n",
    "                  category_index,\n",
    "                  use_normalized_coordinates=True,\n",
    "                  line_thickness=3,\n",
    "                  min_score_thresh=.5)\n",
    "    \n",
    "            tracker_counter += 1\n",
    "            if tracker_counter >= 20: #Number of tracked frames between detections\n",
    "                track = False\n",
    "                tracker_counter = 0\n",
    "            \n",
    "            output_q.put(frame)\n",
    "            \n",
    "        fps.update()\n",
    "        \n",
    "    fps.stop()\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] sampling THREADED frames from webcam...\n",
      "label: person\n",
      "score: 0.530849039555\n",
      "box: [0.40009513 0.21527338 0.9906151  0.8359817 ]\n",
      "label: person\n",
      "score: 0.530849039555\n",
      "box: [0.40009513 0.21527338 0.9906151  0.8359817 ]\n",
      "label: person\n",
      "score: 0.799234867096\n",
      "box: [0.31424657 0.1791535  0.99457395 0.87318844]\n",
      "label: person\n",
      "score: 0.799234867096\n",
      "box: [0.31424657 0.1791535  0.99457395 0.87318844]\n",
      "[INFO] elapsed time (total): 8.45\n",
      "[INFO] approx. FPS: 6.63\n"
     ]
    }
   ],
   "source": [
    "input_q = Queue(maxsize=5)\n",
    "output_q = Queue(maxsize=5)\n",
    "pool = Pool(2, worker, (input_q, output_q))\n",
    "\n",
    "\n",
    "print('[INFO] sampling THREADED frames from webcam...')\n",
    "vs = WebcamVideoStream(src=0,width = 480,height=360).start()\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "\n",
    "out = cv2.VideoWriter('output.avi', fourcc, 5.0, (544, 288),True)\n",
    "frame_counter = 0\n",
    "\n",
    "fps = FPS().start()\n",
    "f_count = 0\n",
    "\n",
    "while True:\n",
    "    frame = vs.read()\n",
    "    input_q.put(frame)\n",
    "\n",
    "    t = time.time()\n",
    "    \n",
    "    if output_q.empty():\n",
    "        pass  # fill up queue\n",
    "    else:\n",
    "        output_rgb = output_q.get()\n",
    "        out.write(output_rgb)\n",
    "        cv2.imshow('Video', output_rgb)\n",
    "        f_count += 1\n",
    "        if f_count >= 50: #For testing.. limiting the number of frames\n",
    "            break\n",
    "        \n",
    "    fps.update()\n",
    "\n",
    "    #print('[INFO] elapsed time: {:.2f}'.format(time.time() - t))\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "fps.stop()\n",
    "print('[INFO] elapsed time (total): {:.2f}'.format(fps.elapsed()))\n",
    "print('[INFO] approx. FPS: {:.2f}'.format(fps.fps()))\n",
    "\n",
    "pool.terminate()\n",
    "vs.stop()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "object_detection_tutorial.ipynb?workspaceId=ronnyvotel:python_inference::citc",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
